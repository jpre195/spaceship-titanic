{"timestamp": 1711393178.554027, "stored_source_code": "# Add description here\n#\n# *Note:* You can open this file as a notebook (JupyterLab: right-click on it in the side bar -> Open With -> Notebook)\n# Uncomment the next two lines to enable auto reloading for imported modules\n# %load_ext autoreload\n# %autoreload 2\n# For more info, see:\n# https://docs.ploomber.io/en/latest/user-guide/faq_index.html#auto-reloading-code-in-jupyter\n# If this task has dependencies, list them them here\n# (e.g. upstream = ['some_task']), otherwise leave as None.\nupstream = ['preprocessing']\n\n# This is a placeholder, leave it as None\nproduct = None\nimport pickle as pkl\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import ConfusionMatrixDisplay, precision_recall_curve, f1_score, PrecisionRecallDisplay\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\ntrain = pd.read_csv(upstream['preprocessing']['train'])\nval = pd.read_csv(upstream['preprocessing']['val'])\n\ntrain.set_index('PassengerId', inplace = True)\nval.set_index('PassengerId', inplace = True)\n\ntrain_X = train.drop('Transported', axis = 1)\ntrain_y = train['Transported']\n\nval_X = val.drop('Transported', axis = 1)\nval_y = val['Transported']\n\npipe = Pipeline([('scaler', StandardScaler()),\n                 ('model', CalibratedClassifierCV(LogisticRegression(class_weight = 'balanced'), method = 'isotonic', cv = 3))])\n\npipe.fit(train_X, train_y)\n\nwith open(product['model'], 'wb') as f:\n\n    pkl.dump(pipe, f)\n\nprobs = pipe.predict_proba(train_X)\nprobs = probs[:, 1]\n\nprobs_val = pipe.predict_proba(val_X)\nprobs_val = probs_val[:, 1]\n\nprecision, recall, thresholds = precision_recall_curve(train_y, probas_pred = probs)\n\nthresholds_df = pd.DataFrame({'precision' : precision[:-1],\n                              'recall' : recall[:-1],\n                              'threshold' : thresholds})\n\nthresholds_df['f1'] = (2 * thresholds_df['precision'] * thresholds_df['recall']) / (thresholds_df['precision'] + thresholds_df['recall'])\nthreshold = thresholds_df[thresholds_df['f1'] == max(thresholds_df['f1'])]['threshold'].values[0]\n\npreds = [1 if prob > threshold else 0 for prob in probs]\npreds_val = [1 if prob > threshold else 0 for prob in probs_val]\n\ndisp = ConfusionMatrixDisplay.from_predictions(train_y, preds)\nplt.savefig(product['confusion'])\nplt.show()\n\ndisp = ConfusionMatrixDisplay.from_predictions(val_y, preds_val)\nplt.savefig(product['confusion_test'])\nplt.show()\n\ndisp = PrecisionRecallDisplay.from_predictions(train_y, probs)\nplt.savefig(product['pr_curve'])\nplt.show()\n\nf1 = f1_score(train_y, preds)\nf1_val = f1_score(val_y, preds_val)\n\nprint(f'Train F1: {f1}')\nprint(f'Val F1: {f1_val}')", "params": {}}